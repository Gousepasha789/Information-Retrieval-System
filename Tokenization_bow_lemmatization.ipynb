{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMR3bGiatMV4Jg3yYlC/rN3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gousepasha789/Information-Retrieval-System/blob/main/Tokenization_bow_lemmatization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "import nltk\n",
        "import spacy\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Download the necessary NLTK data package\n",
        "nltk.download('punkt_tab') # This line is added to download the missing data\n",
        "\n",
        "# Sample text\n",
        "text = \"Tokenization is the process of breaking text into words or sentences. It's an essential step in NLP!\"\n",
        "\n",
        "# ------------------ 1Ô∏è‚É£ WORD TOKENIZATION ------------------\n",
        "# Using NLTK\n",
        "word_tokens = word_tokenize(text)\n",
        "print(\"\\nüîπ Word Tokens (NLTK):\", word_tokens)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTT3bwyAM31i",
        "outputId": "7f98a664-0eca-4eec-b274-3fca16c4f77e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîπ Word Tokens (NLTK): ['Tokenization', 'is', 'the', 'process', 'of', 'breaking', 'text', 'into', 'words', 'or', 'sentences', '.', 'It', \"'s\", 'an', 'essential', 'step', 'in', 'NLP', '!']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ 2Ô∏è‚É£ CHARACTER TOKENIZATION ------------------\n",
        "char_tokens = list(text)  # Splitting text into individual characters\n",
        "print(\"\\nüîπ Character Tokens:\", char_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DErk00BgMQB8",
        "outputId": "cbb9a3b5-1090-44d4-c7aa-cd25c2a7edc8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîπ Character Tokens: ['T', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'i', 's', ' ', 't', 'h', 'e', ' ', 'p', 'r', 'o', 'c', 'e', 's', 's', ' ', 'o', 'f', ' ', 'b', 'r', 'e', 'a', 'k', 'i', 'n', 'g', ' ', 't', 'e', 'x', 't', ' ', 'i', 'n', 't', 'o', ' ', 'w', 'o', 'r', 'd', 's', ' ', 'o', 'r', ' ', 's', 'e', 'n', 't', 'e', 'n', 'c', 'e', 's', '.', ' ', 'I', 't', \"'\", 's', ' ', 'a', 'n', ' ', 'e', 's', 's', 'e', 'n', 't', 'i', 'a', 'l', ' ', 's', 't', 'e', 'p', ' ', 'i', 'n', ' ', 'N', 'L', 'P', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ 3Ô∏è‚É£ SENTENCE TOKENIZATION ------------------\n",
        "# Using NLTK\n",
        "sentence_tokens = sent_tokenize(text)\n",
        "print(\"\\nüîπ Sentence Tokens (NLTK):\", sentence_tokens)\n",
        "\n",
        "\n",
        "# Using spaCy\n",
        "sentence_tokens_spacy = [sent.text for sent in doc.sents]\n",
        "print(\"\\nüîπ Sentence Tokens (spaCy):\", sentence_tokens_spacy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zG3kG8MiMU5o",
        "outputId": "10dba644-c7d4-4406-db3e-c36054cba888"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîπ Sentence Tokens (NLTK): ['Tokenization is the process of breaking text into words or sentences.', \"It's an essential step in NLP!\"]\n",
            "\n",
            "üîπ Sentence Tokens (spaCy): ['Tokenization is the process of breaking text into words or sentences.', \"It's an essential step in NLP!\"]\n"
          ]
        }
      ]
    },
    {
      "source": [
        "from tokenizers import Tokenizer, models, pre_tokenizers, trainers\n",
        "\n",
        "# ------------------ WordPiece Tokenization (Fixed) ------------------\n",
        "wp_tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
        "wp_tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "\n",
        "# ‚úÖ Use a larger vocabulary size to get meaningful subwords\n",
        "wp_trainer = trainers.WordPieceTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"], vocab_size=500)\n",
        "wp_tokenizer.train_from_iterator([text], trainer=wp_trainer)\n",
        "\n",
        "# Tokenize with WordPiece\n",
        "wp_tokens = wp_tokenizer.encode(text).tokens\n",
        "print(\"\\nüîπ WordPiece Tokens (Fixed):\", wp_tokens)\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xBAkT8xPdeK",
        "outputId": "47d2389a-b5ab-4c11-d293-c02b929ac0a8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîπ WordPiece Tokens (Fixed): ['Tokenization', 'is', 'the', 'process', 'of', 'breaking', 'text', 'into', 'words', 'or', 'sentences', '.', 'It', \"'\", 's', 'an', 'essential', 'step', 'in', 'NLP', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ------------------ 4Ô∏è‚É£ BAG OF WORDS (BoW) ------------------\n",
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform([text])\n",
        "\n",
        "# Get feature names (unique words)\n",
        "bow_features = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert BoW matrix to array\n",
        "bow_array = bow_matrix.toarray()\n",
        "\n",
        "print(\"\\nüîπ Bag of Words Features:\", bow_features)\n",
        "print(\"\\nüîπ BoW Matrix:\\n\", bow_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piQ-PYMZMWyZ",
        "outputId": "27cd3dcb-e285-43e0-9348-bc12749206e0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîπ Bag of Words Features: ['an' 'breaking' 'essential' 'in' 'into' 'is' 'it' 'nlp' 'of' 'or'\n",
            " 'process' 'sentences' 'step' 'text' 'the' 'tokenization' 'words']\n",
            "\n",
            "üîπ BoW Matrix:\n",
            " [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Tokenize words\n",
        "word_tokens = word_tokenize(text)\n",
        "\n",
        "# Lemmatize each word\n",
        "lemmatized_words_nltk = [lemmatizer.lemmatize(word) for word in word_tokens]\n",
        "\n",
        "print(\"\\nüîπ Lemmatized Words (NLTK):\", lemmatized_words_nltk)\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjpRnYkFOTev",
        "outputId": "17321c32-a9fb-44fa-ba70-19c63d8d3922"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîπ Lemmatized Words (NLTK): ['Tokenization', 'is', 'the', 'process', 'of', 'breaking', 'text', 'into', 'word', 'or', 'sentence', '.', 'It', \"'s\", 'an', 'essential', 'step', 'in', 'NLP', '!']\n"
          ]
        }
      ]
    }
  ]
}